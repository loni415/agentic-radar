{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AiUCe22S5RR"
   },
   "source": [
    "#Crawl4AI#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OBAoZteVNPDI"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U crawl4ai\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DRVfzwdYNSwf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0.post4\n"
     ]
    }
   ],
   "source": [
    "# Check crawl4ai version\n",
    "import pkg_resources\n",
    "print(pkg_resources.get_distribution(\"crawl4ai\").version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Y9Hy6uiLNZaD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5641.24s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!crawl4ai-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_MbpLO-VNZ4_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5650.58s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INIT].... → Running Crawl4AI health check...\u001b[0m\n",
      "\u001b[36m[INIT].... → Crawl4AI 0.4.3b3\u001b[0m\n",
      "\u001b[36m[TEST].... ℹ Testing crawling capabilities...\u001b[0m\n",
      "\u001b[36m[EXPORT].. ℹ Exporting PDF and taking screenshot took 0.42s\u001b[0m\n",
      "\u001b[32m[FETCH]... ↓ https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Time: 2.39s\u001b[0m\n",
      "\u001b[36m[SCRAPE].. ◆ Processed https://crawl4ai.com... | Time: 9ms\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Total: \u001b[33m2.40s\u001b[0m\u001b[0m\n",
      "\u001b[32m[COMPLETE] ● ✅ Crawling test passed!\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!crawl4ai-doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIVnXqZaNbvL"
   },
   "outputs": [],
   "source": [
    "# If you face with an error try it manually\n",
    "# !playwright install --with-deps chrome # Recommended for Colab/Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af4BY4Zvlf0D"
   },
   "source": [
    "I suggest you first try the code below to ensure that Playwright is installed and works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2c2a74c8"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3383,
     "status": "ok",
     "timestamp": 1735727566446,
     "user": {
      "displayName": "Uncle Code",
      "userId": "00318991760825165788"
     },
     "user_tz": -480
    },
    "id": "by3AVeSwlcba",
    "outputId": "7c2842c2-8b87-4fae-de7d-a80033f7724f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Example Domain\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def test_browser():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto('https://example.com')\n",
    "        print(f'Title: {await page.title()}')\n",
    "        await browser.close()\n",
    "\n",
    "asyncio.run(test_browser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AsyncWebCrawler' from 'crawl4ai' (/Users/lukasfiller/miniforge3/envs/craawl4ai/lib/python3.12/site-packages/crawl4ai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#test 2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcrawl4ai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler() \u001b[38;5;28;01mas\u001b[39;00m crawler:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AsyncWebCrawler' from 'crawl4ai' (/Users/lukasfiller/miniforge3/envs/craawl4ai/lib/python3.12/site-packages/crawl4ai/__init__.py)"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
    "\n",
    "async def main():\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.example.com\",\n",
    "        )\n",
    "        print(result.markdown[:300])  # Show the first 300 characters of extracted text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lukasfiller/miniforge3/envs/craawl4ai/lib/python3.12/site-packages/crawl4ai/__init__.py\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AsyncWebCrawler' from 'crawl4ai' (/Users/lukasfiller/miniforge3/envs/craawl4ai/lib/python3.12/site-packages/crawl4ai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcrawl4ai\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(crawl4ai\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcrawl4ai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncWebCrawler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(AsyncWebCrawler)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AsyncWebCrawler' from 'crawl4ai' (/Users/lukasfiller/miniforge3/envs/craawl4ai/lib/python3.12/site-packages/crawl4ai/__init__.py)"
     ]
    }
   ],
   "source": [
    "import crawl4ai\n",
    "print(crawl4ai.__file__)\n",
    "\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "print(AsyncWebCrawler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3c558d7"
   },
   "source": [
    "#### 2. **Basic Setup and Simple Crawl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3480,
     "status": "ok",
     "timestamp": 1738773168612,
     "user": {
      "displayName": "Uncle Code",
      "userId": "00318991760825165788"
     },
     "user_tz": -480
    },
    "id": "003376f3",
    "outputId": "42695969-299d-4d92-a817-d22ca86ff500"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AsyncWebCrawler' from 'crawl4ai' (/Users/lukasfiller/miniforge3/envs/craawl4ai/lib/python3.12/site-packages/crawl4ai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnest_asyncio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcrawl4ai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncWebCrawler, CacheMode, BrowserConfig, CrawlerRunConfig, CacheMode\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimple_crawl\u001b[39m():\n\u001b[1;32m      8\u001b[0m     crawler_run_config \u001b[38;5;241m=\u001b[39m CrawlerRunConfig( cache_mode\u001b[38;5;241m=\u001b[39mCacheMode\u001b[38;5;241m.\u001b[39mBYPASS)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AsyncWebCrawler' from 'crawl4ai' (/Users/lukasfiller/miniforge3/envs/craawl4ai/lib/python3.12/site-packages/crawl4ai/__init__.py)"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from crawl4ai import AsyncWebCrawler, CacheMode, BrowserConfig, CrawlerRunConfig, CacheMode\n",
    "\n",
    "async def simple_crawl():\n",
    "    crawler_run_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS)\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.kidocode.com/degrees/technology\",\n",
    "            config=crawler_run_config\n",
    "        )\n",
    "        print(result.markdown_v2.raw_markdown[:500].replace(\"\\n\", \" -- \"))  # Print the first 500 characters\n",
    "\n",
    "asyncio.run(simple_crawl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do6Ocue1VLX6"
   },
   "source": [
    "web crawl & scrape\n",
    "https://medium.com/@honeyricky1m3/crawl4ai-automating-web-crawling-and-data-extraction-for-ai-agents-33c9c7ecfa26\n",
    "\n",
    "https://crawl4ai.com/mkdocs/installation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdD1e9WsVNcw"
   },
   "outputs": [],
   "source": [
    "#v1\n",
    "\n",
    "!pip install \"crawl4ai @ git+https://github.com/unclecode/crawl4ai.git\" pydantic\n",
    "#transformers torch nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4567,
     "status": "ok",
     "timestamp": 1725085110892,
     "user": {
      "displayName": "Lukas F",
      "userId": "07341902451649282007"
     },
     "user_tz": 420
    },
    "id": "aaOBfpK-VW4C",
    "outputId": "520e95b6-8df7-4750-e274-428ae31bbddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] 🚀 Initializing LocalSeleniumCrawlerStrategy\n",
      "[LOG] 🌤️  Warming up the WebCrawler\n",
      "[LOG] 🌞 WebCrawler is ready to crawl\n",
      "[LOG] 🚀 Crawling done for https://weibo.com/u/7815132498, success: True, time taken: 2.5049993991851807 seconds\n",
      "[ERROR] 🚫 Failed to crawl https://weibo.com/u/7815132498, error: can only concatenate str (not \"NoneType\") to str\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#v1\n",
    "\n",
    "from crawl4ai import WebCrawler\n",
    "\n",
    "# Create an instance of WebCrawler\n",
    "crawler = WebCrawler()\n",
    "\n",
    "# Warm up the crawler (load necessary models)\n",
    "crawler.warmup()\n",
    "\n",
    "# Run the crawler on a URL\n",
    "result = crawler.run(url=\"https://weibo.com/u/7815132498\")\n",
    "\n",
    "# Print the extracted content\n",
    "print(result.markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvwD9vDGVZba"
   },
   "outputs": [],
   "source": [
    "#v1\n",
    "import os\n",
    "from crawl4ai import WebCrawler\n",
    "from crawl4ai.extraction_strategy import LLMExtractionStrategy\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class OpenAIModelFee(BaseModel):\n",
    "    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n",
    "    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n",
    "    output_fee: str = Field(..., description=\"Fee for output token ßfor the OpenAI model.\")\n",
    "\n",
    "url = 'https://openai.com/api/pricing/'\n",
    "crawler = WebCrawler()\n",
    "crawler.warmup()\n",
    "\n",
    "result = crawler.run(\n",
    "        url=url,\n",
    "        word_count_threshold=1,\n",
    "        extraction_strategy= LLMExtractionStrategy(\n",
    "            provider= \"openai/gpt-4o\", api_token = os.getenv('OPENAI_API_KEY'),\n",
    "            schema=OpenAIModelFee.schema(),\n",
    "            extraction_type=\"schema\",\n",
    "            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens.\n",
    "            Do not miss any models in the entire content. One extracted model JSON format should look like this:\n",
    "            {\"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\"}.\"\"\"\n",
    "        ),\n",
    "        bypass_cache=True,\n",
    "    )\n",
    "\n",
    "print(result.extracted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc2OjxttTGws"
   },
   "outputs": [],
   "source": [
    "#v2\n",
    "#%%captured\n",
    "!pip install \"crawl4ai @ git+https://github.com/unclecode/crawl4ai.git@main-75\" pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16379,
     "status": "ok",
     "timestamp": 1725084519026,
     "user": {
      "displayName": "Lukas F",
      "userId": "07341902451649282007"
     },
     "user_tz": 420
    },
    "id": "Ahe0171jTo-U",
    "outputId": "f6bfc0e2-a1c0-4e9c-8085-35ca88a2b3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] 🚀 Initializing LocalSeleniumCrawlerStrategy\n",
      "[LOG] 🌤️  Warming up the WebCrawler\n",
      "[LOG] 🌞 WebCrawler is ready to crawl\n"
     ]
    }
   ],
   "source": [
    "import time, os, sys\n",
    "import json\n",
    "from crawl4ai.chunking_strategy import *\n",
    "from crawl4ai.extraction_strategy import *\n",
    "from crawl4ai.crawler_strategy import *\n",
    "from crawl4ai.web_crawler import WebCrawler\n",
    "from crawl4ai import config\n",
    "crawler = WebCrawler()\n",
    "crawler.warmup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11714,
     "status": "ok",
     "timestamp": 1725084554694,
     "user": {
      "displayName": "Lukas F",
      "userId": "07341902451649282007"
     },
     "user_tz": 420
    },
    "id": "jjMxus83TuU1",
    "outputId": "edf51b48-3fac-4e5c-a9fb-95046bfb1e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] 🚀 Crawling done for https://seeherswim.com, success: True, time taken: 11.476678133010864 seconds\n",
      "[LOG] 🚀 Content extracted for https://seeherswim.com, success: True, time taken: 0.019153118133544922 seconds\n",
      "[LOG] 🔥 Extracting semantic blocks for https://seeherswim.com, Strategy: NoExtractionStrategy\n",
      "[LOG] 🚀 Extraction done for https://seeherswim.com, time taken: 0.01941514015197754 seconds.\n",
      "dict_keys(['url', 'html', 'success', 'cleaned_html', 'media', 'links', 'screenshot', 'markdown', 'extracted_content', 'metadata', 'error_message'])\n",
      "Skip to content\n",
      "\n",
      "FREE SHIPPING ON ORDERS $100+\n",
      "\n",
      "  * SHOP __\n",
      "    * Performance Swim\n",
      "    * Accessories\n"
     ]
    }
   ],
   "source": [
    "#simple example...works\n",
    "url = r\"https://seeherswim.com\"\n",
    "result = crawler.run(url, word_count_threshold=10, bypass_cache=True)\n",
    "print(result.model_dump().keys())\n",
    "print(result.markdown[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zS1WwXddW4xi"
   },
   "outputs": [],
   "source": [
    "result = crawler.run(\n",
    "        url=url,\n",
    "        extraction_strategy=LLMExtractionStrategy(\n",
    "            provider=\"ollama/llama3.1\",\n",
    "            api_token=\"no-token\",\n",
    "            instruction=\"Extract only content related to PLM\"\n",
    "        ),\n",
    "       bypass_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyZKgOAIVcZC"
   },
   "outputs": [],
   "source": [
    "!pip install praisonai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9I1wEpPVgk4"
   },
   "outputs": [],
   "source": [
    "# tools.py\n",
    "import os\n",
    "from crawl4ai import WebCrawler\n",
    "from crawl4ai.extraction_strategy import LLMExtractionStrategy\n",
    "from pydantic import BaseModel, Field\n",
    "from praisonai_tools import BaseTool\n",
    "\n",
    "class ModelFee(BaseModel):\n",
    "    llm_model_name: str = Field(..., description=\"Name of the model.\")\n",
    "    input_fee: str = Field(..., description=\"Fee for input token for the model.\")\n",
    "    output_fee: str = Field(..., description=\"Fee for output token for the model.\")\n",
    "\n",
    "class ModelFeeTool(BaseTool):\n",
    "    name: str = \"ModelFeeTool\"\n",
    "    description: str = \"Extracts model fees for input and output tokens from the given pricing page.\"\n",
    "\n",
    "    def _run(self, url: str):\n",
    "        crawler = WebCrawler()\n",
    "        crawler.warmup()\n",
    "\n",
    "        result = crawler.run(\n",
    "            url=url,\n",
    "            word_count_threshold=1,\n",
    "            extraction_strategy= LLMExtractionStrategy(\n",
    "                provider=\"openai/gpt-4o\",\n",
    "                api_token=os.getenv('OPENAI_API_KEY'),\n",
    "                schema=ModelFee.schema(),\n",
    "                extraction_type=\"schema\",\n",
    "                instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens.\n",
    "                Do not miss any models in the entire content. One extracted model JSON format should look like this:\n",
    "                {\"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\"}.\"\"\"\n",
    "            ),\n",
    "            bypass_cache=True,\n",
    "        )\n",
    "        return result.extracted_content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the ModelFeeTool\n",
    "    tool = ModelFeeTool()\n",
    "    url = \"https://www.openai.com/pricing\"\n",
    "    result = tool.run(url)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpJ8XZBlVjbC"
   },
   "outputs": [],
   "source": [
    "framework: crewai\n",
    "topic: extract model pricing from websites\n",
    "roles:\n",
    "  web_scraper:\n",
    "    backstory: An expert in web scraping with a deep understanding of extracting structured\n",
    "      data from online sources. https://openai.com/api/pricing/ https://www.anthropic.com/pricing https://cohere.com/pricing\n",
    "    goal: Gather model pricing data from various websites\n",
    "    role: Web Scraper\n",
    "    tasks:\n",
    "      scrape_model_pricing:\n",
    "        description: Scrape model pricing information from the provided list of websites.\n",
    "        expected_output: Raw HTML or JSON containing model pricing data.\n",
    "    tools:\n",
    "    - 'ModelFeeTool'\n",
    "  data_cleaner:\n",
    "    backstory: Specialist in data cleaning, ensuring that all collected data is accurate\n",
    "      and properly formatted.\n",
    "    goal: Clean and organize the scraped pricing data\n",
    "    role: Data Cleaner\n",
    "    tasks:\n",
    "      clean_pricing_data:\n",
    "        description: Process the raw scraped data to remove any duplicates and inconsistencies,\n",
    "          and convert it into a structured format.\n",
    "        expected_output: Cleaned and organized JSON or CSV file with model pricing\n",
    "          data.\n",
    "    tools:\n",
    "    - ''\n",
    "  data_analyzer:\n",
    "    backstory: Data analysis expert focused on deriving actionable insights from structured\n",
    "      data.\n",
    "    goal: Analyze the cleaned pricing data to extract insights\n",
    "    role: Data Analyzer\n",
    "    tasks:\n",
    "      analyze_pricing_data:\n",
    "        description: Analyze the cleaned data to extract trends, patterns, and insights\n",
    "          on model pricing.\n",
    "        expected_output: Detailed report summarizing model pricing trends and insights.\n",
    "    tools:\n",
    "    - ''\n",
    "dependencies: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAnog6kfSlDd"
   },
   "outputs": [],
   "source": [
    "# 4.1 Multi-query prompting\n",
    "multi_query_prompt = \"\"\"\n",
    "You are an AI language model assistant.\n",
    "Your task is to create five versions of the user's question to fetch documents from a vector database.\n",
    "By offering multiple perspectives on the user's question, your goal is to assist the user in overcoming some of the restrictions of distance-based similarity search.\n",
    "Give these alternative questions, each on a new line.\n",
    "Question: {question}\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5CKvrjKSvLY"
   },
   "outputs": [],
   "source": [
    "# 4.1 Decomposition prompting\n",
    "decomposition_template = \"\"\"You are an AI language assistant.\n",
    "Your task break the following question into 5 sub questions.\n",
    "By doing so, you're helping the user construct the final answer progressively.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DghXA5bS2s-"
   },
   "outputs": [],
   "source": [
    "# 4.1 HyDE prompting\n",
    "hyde_prompt = \"\"\"\n",
    "You're an AI language assistant.\n",
    "Your task is to generate a more broader version of the question below.\n",
    "By doing so, you're helping the user with more information.\n",
    "Don't explain the question. Only provide a more broader version of it.\n",
    "Question: {question}\n",
    "Output:\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNXafrmRjkvRGluYj/u5EV5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "craawl4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
